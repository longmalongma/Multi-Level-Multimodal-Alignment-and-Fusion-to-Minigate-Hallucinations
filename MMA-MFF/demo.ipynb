{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large RAM is required to load the larger models. Running on GPU can optimize inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/malmm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/malmm/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/opt/conda/envs/malmm/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "import decord\n",
    "from decord import VideoReader\n",
    "from decord import cpu, gpu\n",
    "decord.bridge.set_bridge('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load an example video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_duration: 70.3, fps: 24.0\n"
     ]
    }
   ],
   "source": [
    "def load_video(vr, start_time, end_time, fps, num_frames=20):\n",
    "    start_index = int(round(start_time * fps))\n",
    "    end_index = int(round(end_time * fps))\n",
    "    select_frame_index = np.rint(np.linspace(start_index, end_index-1, num_frames)).astype(int).tolist()\n",
    "    frames = vr.get_batch(select_frame_index).permute(3, 0, 1, 2).to(torch.float32)\n",
    "    return frames\n",
    "\n",
    "file_path = \"example/video.mp4\"\n",
    "vr = VideoReader(file_path, ctx=cpu(0))\n",
    "total_frames = len(vr)\n",
    "fps = vr.get_avg_fps()\n",
    "duration = total_frames / fps\n",
    "\n",
    "print(\"video_duration: {:.1f}, fps: {:.1f}\".format(duration, fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# setup device to use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the full video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"example/video.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Embed video using HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"example/video.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained InstructBlip model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W127 00:16:58.848512581 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "/t-ng/Deng/py/MMA-LLM/lavis/models/eva_vit.py:440: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file, map_location=\"cpu\")\n",
      "/opt/conda/envs/malmm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.00it/s]\n",
      "/t-ng/Deng/py/MMA-LLM/load_clip_pretrain.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(pretrained_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model key: query_tokens\n",
      "model key: ln_vision.weight\n",
      "model key: ln_vision.bias\n",
      "model key: clip_text_embedding.position_ids\n",
      "model key: clip_text_embedding.word_embeddings.weight\n",
      "model key: clip_text_embedding.position_embeddings.weight\n",
      "model key: clip_text_embedding.LayerNorm.weight\n",
      "model key: clip_text_embedding.LayerNorm.bias\n",
      "model key: T1.text_projection\n",
      "model key: T1.qformer_projection\n",
      "model key: T1.logit_scale\n",
      "model key: T1.text_embedding.position_ids\n",
      "model key: T1.text_embedding.word_embeddings.weight\n",
      "model key: T1.text_embedding.position_embeddings.weight\n",
      "model key: T1.text_embedding.LayerNorm.weight\n",
      "model key: T1.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.embeddings.position_ids\n",
      "model key: Qformer.bert.embeddings.word_embeddings.weight\n",
      "model key: Qformer.bert.embeddings.position_embeddings.weight\n",
      "model key: Qformer.bert.embeddings.LayerNorm.weight\n",
      "model key: Qformer.bert.embeddings.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.0.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.0.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.0.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.0.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.0.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.0.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.0.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.0.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.0.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.0.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.0.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.0.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.0.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.0.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.0.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.0.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.0.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.1.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.1.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.1.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.1.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.1.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.1.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.1.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.1.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.1.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.1.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.1.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.1.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.1.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.1.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.1.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.1.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.1.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.2.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.2.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.2.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.2.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.2.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.2.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.2.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.2.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.2.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.2.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.2.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.2.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.2.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.2.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.2.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.2.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.2.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.2.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.2.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.3.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.3.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.3.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.3.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.3.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.3.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.3.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.3.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.3.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.3.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.3.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.3.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.3.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.3.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.3.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.3.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.3.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.3.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.3.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.4.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.4.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.4.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.4.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.4.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.4.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.4.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.4.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.4.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.4.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.4.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.4.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.4.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.4.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.4.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.4.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.4.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.4.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.4.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.5.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.5.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.5.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.5.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.5.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.5.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.5.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.5.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.5.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.5.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.5.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.5.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.5.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.5.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.5.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.5.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.5.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.5.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.5.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.6.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.6.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.6.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.6.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.6.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.6.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.6.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.6.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.6.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.6.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.6.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.6.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.6.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.6.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.6.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.6.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.6.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.6.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.6.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.7.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.7.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.7.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.7.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.7.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.7.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.7.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.7.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.7.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.7.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.7.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.7.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.7.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.7.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.7.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.7.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.7.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.7.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.7.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.8.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.8.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.8.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.8.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.8.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.8.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.8.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.8.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.8.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.8.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.8.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.8.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.8.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.8.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.8.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.8.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.8.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.8.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.8.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.9.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.9.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.9.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.9.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.9.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.9.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.9.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.9.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.9.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.9.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.9.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.9.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.9.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.9.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.9.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.9.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.9.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.9.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.9.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.10.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.10.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.10.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.10.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.10.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.10.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.10.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.10.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.10.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.10.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.10.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.10.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.10.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.10.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.10.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.10.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.10.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.10.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.10.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.11.attention.self.query.weight\n",
      "model key: Qformer.bert.encoder.layer.11.attention.self.query.bias\n",
      "model key: Qformer.bert.encoder.layer.11.attention.self.key.weight\n",
      "model key: Qformer.bert.encoder.layer.11.attention.self.key.bias\n",
      "model key: Qformer.bert.encoder.layer.11.attention.self.value.weight\n",
      "model key: Qformer.bert.encoder.layer.11.attention.self.value.bias\n",
      "model key: Qformer.bert.encoder.layer.11.attention.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.11.attention.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.11.intermediate.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.11.intermediate.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.11.output.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.11.output.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.11.intermediate_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.11.intermediate_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.11.output_query.dense.weight\n",
      "model key: Qformer.bert.encoder.layer.11.output_query.dense.bias\n",
      "model key: Qformer.bert.encoder.layer.11.output_query.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.11.output_query.LayerNorm.bias\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.text_projection\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.qformer_projection\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.logit_scale\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.text_embedding.position_ids\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.text_embedding.word_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.text_embedding.position_embeddings.weight\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.text_embedding.LayerNorm.weight\n",
      "model key: Qformer.bert.encoder.layer.11.clip4qformer.text_embedding.LayerNorm.bias\n",
      "model key: llm_model.model.layers.0.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.1.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.2.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.3.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.4.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.5.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.6.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.7.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.8.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.9.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.10.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.11.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.12.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.13.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.14.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.15.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.16.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.17.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.18.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.19.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.20.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.21.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.22.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.23.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.24.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.25.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.26.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.27.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.28.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.29.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.30.self_attn.rotary_emb.inv_freq\n",
      "model key: llm_model.model.layers.31.self_attn.rotary_emb.inv_freq\n",
      "model key: image_pe.weight\n",
      "loaded step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/t-ng/Deng/py/MMA-LLM/lavis/models/blip2_models/blip2_clip.py:250: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n",
      "/t-ng/Deng/py/MMA-LLM/lavis/models/base_model.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(url_or_filename, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['visual_encoder.cls_token', 'visual_encoder.pos_embed', 'visual_encoder.patch_embed.proj.weight', 'visual_encoder.patch_embed.proj.bias', 'visual_encoder.blocks.0.norm1.weight', 'visual_encoder.blocks.0.norm1.bias', 'visual_encoder.blocks.0.attn.q_bias', 'visual_encoder.blocks.0.attn.v_bias', 'visual_encoder.blocks.0.attn.qkv.weight', 'visual_encoder.blocks.0.attn.proj.weight', 'visual_encoder.blocks.0.attn.proj.bias', 'visual_encoder.blocks.0.norm2.weight', 'visual_encoder.blocks.0.norm2.bias', 'visual_encoder.blocks.0.mlp.fc1.weight', 'visual_encoder.blocks.0.mlp.fc1.bias', 'visual_encoder.blocks.0.mlp.fc2.weight', 'visual_encoder.blocks.0.mlp.fc2.bias', 'visual_encoder.blocks.1.norm1.weight', 'visual_encoder.blocks.1.norm1.bias', 'visual_encoder.blocks.1.attn.q_bias', 'visual_encoder.blocks.1.attn.v_bias', 'visual_encoder.blocks.1.attn.qkv.weight', 'visual_encoder.blocks.1.attn.proj.weight', 'visual_encoder.blocks.1.attn.proj.bias', 'visual_encoder.blocks.1.norm2.weight', 'visual_encoder.blocks.1.norm2.bias', 'visual_encoder.blocks.1.mlp.fc1.weight', 'visual_encoder.blocks.1.mlp.fc1.bias', 'visual_encoder.blocks.1.mlp.fc2.weight', 'visual_encoder.blocks.1.mlp.fc2.bias', 'visual_encoder.blocks.2.norm1.weight', 'visual_encoder.blocks.2.norm1.bias', 'visual_encoder.blocks.2.attn.q_bias', 'visual_encoder.blocks.2.attn.v_bias', 'visual_encoder.blocks.2.attn.qkv.weight', 'visual_encoder.blocks.2.attn.proj.weight', 'visual_encoder.blocks.2.attn.proj.bias', 'visual_encoder.blocks.2.norm2.weight', 'visual_encoder.blocks.2.norm2.bias', 'visual_encoder.blocks.2.mlp.fc1.weight', 'visual_encoder.blocks.2.mlp.fc1.bias', 'visual_encoder.blocks.2.mlp.fc2.weight', 'visual_encoder.blocks.2.mlp.fc2.bias', 'visual_encoder.blocks.3.norm1.weight', 'visual_encoder.blocks.3.norm1.bias', 'visual_encoder.blocks.3.attn.q_bias', 'visual_encoder.blocks.3.attn.v_bias', 'visual_encoder.blocks.3.attn.qkv.weight', 'visual_encoder.blocks.3.attn.proj.weight', 'visual_encoder.blocks.3.attn.proj.bias', 'visual_encoder.blocks.3.norm2.weight', 'visual_encoder.blocks.3.norm2.bias', 'visual_encoder.blocks.3.mlp.fc1.weight', 'visual_encoder.blocks.3.mlp.fc1.bias', 'visual_encoder.blocks.3.mlp.fc2.weight', 'visual_encoder.blocks.3.mlp.fc2.bias', 'visual_encoder.blocks.4.norm1.weight', 'visual_encoder.blocks.4.norm1.bias', 'visual_encoder.blocks.4.attn.q_bias', 'visual_encoder.blocks.4.attn.v_bias', 'visual_encoder.blocks.4.attn.qkv.weight', 'visual_encoder.blocks.4.attn.proj.weight', 'visual_encoder.blocks.4.attn.proj.bias', 'visual_encoder.blocks.4.norm2.weight', 'visual_encoder.blocks.4.norm2.bias', 'visual_encoder.blocks.4.mlp.fc1.weight', 'visual_encoder.blocks.4.mlp.fc1.bias', 'visual_encoder.blocks.4.mlp.fc2.weight', 'visual_encoder.blocks.4.mlp.fc2.bias', 'visual_encoder.blocks.5.norm1.weight', 'visual_encoder.blocks.5.norm1.bias', 'visual_encoder.blocks.5.attn.q_bias', 'visual_encoder.blocks.5.attn.v_bias', 'visual_encoder.blocks.5.attn.qkv.weight', 'visual_encoder.blocks.5.attn.proj.weight', 'visual_encoder.blocks.5.attn.proj.bias', 'visual_encoder.blocks.5.norm2.weight', 'visual_encoder.blocks.5.norm2.bias', 'visual_encoder.blocks.5.mlp.fc1.weight', 'visual_encoder.blocks.5.mlp.fc1.bias', 'visual_encoder.blocks.5.mlp.fc2.weight', 'visual_encoder.blocks.5.mlp.fc2.bias', 'visual_encoder.blocks.6.norm1.weight', 'visual_encoder.blocks.6.norm1.bias', 'visual_encoder.blocks.6.attn.q_bias', 'visual_encoder.blocks.6.attn.v_bias', 'visual_encoder.blocks.6.attn.qkv.weight', 'visual_encoder.blocks.6.attn.proj.weight', 'visual_encoder.blocks.6.attn.proj.bias', 'visual_encoder.blocks.6.norm2.weight', 'visual_encoder.blocks.6.norm2.bias', 'visual_encoder.blocks.6.mlp.fc1.weight', 'visual_encoder.blocks.6.mlp.fc1.bias', 'visual_encoder.blocks.6.mlp.fc2.weight', 'visual_encoder.blocks.6.mlp.fc2.bias', 'visual_encoder.blocks.7.norm1.weight', 'visual_encoder.blocks.7.norm1.bias', 'visual_encoder.blocks.7.attn.q_bias', 'visual_encoder.blocks.7.attn.v_bias', 'visual_encoder.blocks.7.attn.qkv.weight', 'visual_encoder.blocks.7.attn.proj.weight', 'visual_encoder.blocks.7.attn.proj.bias', 'visual_encoder.blocks.7.norm2.weight', 'visual_encoder.blocks.7.norm2.bias', 'visual_encoder.blocks.7.mlp.fc1.weight', 'visual_encoder.blocks.7.mlp.fc1.bias', 'visual_encoder.blocks.7.mlp.fc2.weight', 'visual_encoder.blocks.7.mlp.fc2.bias', 'visual_encoder.blocks.8.norm1.weight', 'visual_encoder.blocks.8.norm1.bias', 'visual_encoder.blocks.8.attn.q_bias', 'visual_encoder.blocks.8.attn.v_bias', 'visual_encoder.blocks.8.attn.qkv.weight', 'visual_encoder.blocks.8.attn.proj.weight', 'visual_encoder.blocks.8.attn.proj.bias', 'visual_encoder.blocks.8.norm2.weight', 'visual_encoder.blocks.8.norm2.bias', 'visual_encoder.blocks.8.mlp.fc1.weight', 'visual_encoder.blocks.8.mlp.fc1.bias', 'visual_encoder.blocks.8.mlp.fc2.weight', 'visual_encoder.blocks.8.mlp.fc2.bias', 'visual_encoder.blocks.9.norm1.weight', 'visual_encoder.blocks.9.norm1.bias', 'visual_encoder.blocks.9.attn.q_bias', 'visual_encoder.blocks.9.attn.v_bias', 'visual_encoder.blocks.9.attn.qkv.weight', 'visual_encoder.blocks.9.attn.proj.weight', 'visual_encoder.blocks.9.attn.proj.bias', 'visual_encoder.blocks.9.norm2.weight', 'visual_encoder.blocks.9.norm2.bias', 'visual_encoder.blocks.9.mlp.fc1.weight', 'visual_encoder.blocks.9.mlp.fc1.bias', 'visual_encoder.blocks.9.mlp.fc2.weight', 'visual_encoder.blocks.9.mlp.fc2.bias', 'visual_encoder.blocks.10.norm1.weight', 'visual_encoder.blocks.10.norm1.bias', 'visual_encoder.blocks.10.attn.q_bias', 'visual_encoder.blocks.10.attn.v_bias', 'visual_encoder.blocks.10.attn.qkv.weight', 'visual_encoder.blocks.10.attn.proj.weight', 'visual_encoder.blocks.10.attn.proj.bias', 'visual_encoder.blocks.10.norm2.weight', 'visual_encoder.blocks.10.norm2.bias', 'visual_encoder.blocks.10.mlp.fc1.weight', 'visual_encoder.blocks.10.mlp.fc1.bias', 'visual_encoder.blocks.10.mlp.fc2.weight', 'visual_encoder.blocks.10.mlp.fc2.bias', 'visual_encoder.blocks.11.norm1.weight', 'visual_encoder.blocks.11.norm1.bias', 'visual_encoder.blocks.11.attn.q_bias', 'visual_encoder.blocks.11.attn.v_bias', 'visual_encoder.blocks.11.attn.qkv.weight', 'visual_encoder.blocks.11.attn.proj.weight', 'visual_encoder.blocks.11.attn.proj.bias', 'visual_encoder.blocks.11.norm2.weight', 'visual_encoder.blocks.11.norm2.bias', 'visual_encoder.blocks.11.mlp.fc1.weight', 'visual_encoder.blocks.11.mlp.fc1.bias', 'visual_encoder.blocks.11.mlp.fc2.weight', 'visual_encoder.blocks.11.mlp.fc2.bias', 'visual_encoder.blocks.12.norm1.weight', 'visual_encoder.blocks.12.norm1.bias', 'visual_encoder.blocks.12.attn.q_bias', 'visual_encoder.blocks.12.attn.v_bias', 'visual_encoder.blocks.12.attn.qkv.weight', 'visual_encoder.blocks.12.attn.proj.weight', 'visual_encoder.blocks.12.attn.proj.bias', 'visual_encoder.blocks.12.norm2.weight', 'visual_encoder.blocks.12.norm2.bias', 'visual_encoder.blocks.12.mlp.fc1.weight', 'visual_encoder.blocks.12.mlp.fc1.bias', 'visual_encoder.blocks.12.mlp.fc2.weight', 'visual_encoder.blocks.12.mlp.fc2.bias', 'visual_encoder.blocks.13.norm1.weight', 'visual_encoder.blocks.13.norm1.bias', 'visual_encoder.blocks.13.attn.q_bias', 'visual_encoder.blocks.13.attn.v_bias', 'visual_encoder.blocks.13.attn.qkv.weight', 'visual_encoder.blocks.13.attn.proj.weight', 'visual_encoder.blocks.13.attn.proj.bias', 'visual_encoder.blocks.13.norm2.weight', 'visual_encoder.blocks.13.norm2.bias', 'visual_encoder.blocks.13.mlp.fc1.weight', 'visual_encoder.blocks.13.mlp.fc1.bias', 'visual_encoder.blocks.13.mlp.fc2.weight', 'visual_encoder.blocks.13.mlp.fc2.bias', 'visual_encoder.blocks.14.norm1.weight', 'visual_encoder.blocks.14.norm1.bias', 'visual_encoder.blocks.14.attn.q_bias', 'visual_encoder.blocks.14.attn.v_bias', 'visual_encoder.blocks.14.attn.qkv.weight', 'visual_encoder.blocks.14.attn.proj.weight', 'visual_encoder.blocks.14.attn.proj.bias', 'visual_encoder.blocks.14.norm2.weight', 'visual_encoder.blocks.14.norm2.bias', 'visual_encoder.blocks.14.mlp.fc1.weight', 'visual_encoder.blocks.14.mlp.fc1.bias', 'visual_encoder.blocks.14.mlp.fc2.weight', 'visual_encoder.blocks.14.mlp.fc2.bias', 'visual_encoder.blocks.15.norm1.weight', 'visual_encoder.blocks.15.norm1.bias', 'visual_encoder.blocks.15.attn.q_bias', 'visual_encoder.blocks.15.attn.v_bias', 'visual_encoder.blocks.15.attn.qkv.weight', 'visual_encoder.blocks.15.attn.proj.weight', 'visual_encoder.blocks.15.attn.proj.bias', 'visual_encoder.blocks.15.norm2.weight', 'visual_encoder.blocks.15.norm2.bias', 'visual_encoder.blocks.15.mlp.fc1.weight', 'visual_encoder.blocks.15.mlp.fc1.bias', 'visual_encoder.blocks.15.mlp.fc2.weight', 'visual_encoder.blocks.15.mlp.fc2.bias', 'visual_encoder.blocks.16.norm1.weight', 'visual_encoder.blocks.16.norm1.bias', 'visual_encoder.blocks.16.attn.q_bias', 'visual_encoder.blocks.16.attn.v_bias', 'visual_encoder.blocks.16.attn.qkv.weight', 'visual_encoder.blocks.16.attn.proj.weight', 'visual_encoder.blocks.16.attn.proj.bias', 'visual_encoder.blocks.16.norm2.weight', 'visual_encoder.blocks.16.norm2.bias', 'visual_encoder.blocks.16.mlp.fc1.weight', 'visual_encoder.blocks.16.mlp.fc1.bias', 'visual_encoder.blocks.16.mlp.fc2.weight', 'visual_encoder.blocks.16.mlp.fc2.bias', 'visual_encoder.blocks.17.norm1.weight', 'visual_encoder.blocks.17.norm1.bias', 'visual_encoder.blocks.17.attn.q_bias', 'visual_encoder.blocks.17.attn.v_bias', 'visual_encoder.blocks.17.attn.qkv.weight', 'visual_encoder.blocks.17.attn.proj.weight', 'visual_encoder.blocks.17.attn.proj.bias', 'visual_encoder.blocks.17.norm2.weight', 'visual_encoder.blocks.17.norm2.bias', 'visual_encoder.blocks.17.mlp.fc1.weight', 'visual_encoder.blocks.17.mlp.fc1.bias', 'visual_encoder.blocks.17.mlp.fc2.weight', 'visual_encoder.blocks.17.mlp.fc2.bias', 'visual_encoder.blocks.18.norm1.weight', 'visual_encoder.blocks.18.norm1.bias', 'visual_encoder.blocks.18.attn.q_bias', 'visual_encoder.blocks.18.attn.v_bias', 'visual_encoder.blocks.18.attn.qkv.weight', 'visual_encoder.blocks.18.attn.proj.weight', 'visual_encoder.blocks.18.attn.proj.bias', 'visual_encoder.blocks.18.norm2.weight', 'visual_encoder.blocks.18.norm2.bias', 'visual_encoder.blocks.18.mlp.fc1.weight', 'visual_encoder.blocks.18.mlp.fc1.bias', 'visual_encoder.blocks.18.mlp.fc2.weight', 'visual_encoder.blocks.18.mlp.fc2.bias', 'visual_encoder.blocks.19.norm1.weight', 'visual_encoder.blocks.19.norm1.bias', 'visual_encoder.blocks.19.attn.q_bias', 'visual_encoder.blocks.19.attn.v_bias', 'visual_encoder.blocks.19.attn.qkv.weight', 'visual_encoder.blocks.19.attn.proj.weight', 'visual_encoder.blocks.19.attn.proj.bias', 'visual_encoder.blocks.19.norm2.weight', 'visual_encoder.blocks.19.norm2.bias', 'visual_encoder.blocks.19.mlp.fc1.weight', 'visual_encoder.blocks.19.mlp.fc1.bias', 'visual_encoder.blocks.19.mlp.fc2.weight', 'visual_encoder.blocks.19.mlp.fc2.bias', 'visual_encoder.blocks.20.norm1.weight', 'visual_encoder.blocks.20.norm1.bias', 'visual_encoder.blocks.20.attn.q_bias', 'visual_encoder.blocks.20.attn.v_bias', 'visual_encoder.blocks.20.attn.qkv.weight', 'visual_encoder.blocks.20.attn.proj.weight', 'visual_encoder.blocks.20.attn.proj.bias', 'visual_encoder.blocks.20.norm2.weight', 'visual_encoder.blocks.20.norm2.bias', 'visual_encoder.blocks.20.mlp.fc1.weight', 'visual_encoder.blocks.20.mlp.fc1.bias', 'visual_encoder.blocks.20.mlp.fc2.weight', 'visual_encoder.blocks.20.mlp.fc2.bias', 'visual_encoder.blocks.21.norm1.weight', 'visual_encoder.blocks.21.norm1.bias', 'visual_encoder.blocks.21.attn.q_bias', 'visual_encoder.blocks.21.attn.v_bias', 'visual_encoder.blocks.21.attn.qkv.weight', 'visual_encoder.blocks.21.attn.proj.weight', 'visual_encoder.blocks.21.attn.proj.bias', 'visual_encoder.blocks.21.norm2.weight', 'visual_encoder.blocks.21.norm2.bias', 'visual_encoder.blocks.21.mlp.fc1.weight', 'visual_encoder.blocks.21.mlp.fc1.bias', 'visual_encoder.blocks.21.mlp.fc2.weight', 'visual_encoder.blocks.21.mlp.fc2.bias', 'visual_encoder.blocks.22.norm1.weight', 'visual_encoder.blocks.22.norm1.bias', 'visual_encoder.blocks.22.attn.q_bias', 'visual_encoder.blocks.22.attn.v_bias', 'visual_encoder.blocks.22.attn.qkv.weight', 'visual_encoder.blocks.22.attn.proj.weight', 'visual_encoder.blocks.22.attn.proj.bias', 'visual_encoder.blocks.22.norm2.weight', 'visual_encoder.blocks.22.norm2.bias', 'visual_encoder.blocks.22.mlp.fc1.weight', 'visual_encoder.blocks.22.mlp.fc1.bias', 'visual_encoder.blocks.22.mlp.fc2.weight', 'visual_encoder.blocks.22.mlp.fc2.bias', 'visual_encoder.blocks.23.norm1.weight', 'visual_encoder.blocks.23.norm1.bias', 'visual_encoder.blocks.23.attn.q_bias', 'visual_encoder.blocks.23.attn.v_bias', 'visual_encoder.blocks.23.attn.qkv.weight', 'visual_encoder.blocks.23.attn.proj.weight', 'visual_encoder.blocks.23.attn.proj.bias', 'visual_encoder.blocks.23.norm2.weight', 'visual_encoder.blocks.23.norm2.bias', 'visual_encoder.blocks.23.mlp.fc1.weight', 'visual_encoder.blocks.23.mlp.fc1.bias', 'visual_encoder.blocks.23.mlp.fc2.weight', 'visual_encoder.blocks.23.mlp.fc2.bias', 'visual_encoder.blocks.24.norm1.weight', 'visual_encoder.blocks.24.norm1.bias', 'visual_encoder.blocks.24.attn.q_bias', 'visual_encoder.blocks.24.attn.v_bias', 'visual_encoder.blocks.24.attn.qkv.weight', 'visual_encoder.blocks.24.attn.proj.weight', 'visual_encoder.blocks.24.attn.proj.bias', 'visual_encoder.blocks.24.norm2.weight', 'visual_encoder.blocks.24.norm2.bias', 'visual_encoder.blocks.24.mlp.fc1.weight', 'visual_encoder.blocks.24.mlp.fc1.bias', 'visual_encoder.blocks.24.mlp.fc2.weight', 'visual_encoder.blocks.24.mlp.fc2.bias', 'visual_encoder.blocks.25.norm1.weight', 'visual_encoder.blocks.25.norm1.bias', 'visual_encoder.blocks.25.attn.q_bias', 'visual_encoder.blocks.25.attn.v_bias', 'visual_encoder.blocks.25.attn.qkv.weight', 'visual_encoder.blocks.25.attn.proj.weight', 'visual_encoder.blocks.25.attn.proj.bias', 'visual_encoder.blocks.25.norm2.weight', 'visual_encoder.blocks.25.norm2.bias', 'visual_encoder.blocks.25.mlp.fc1.weight', 'visual_encoder.blocks.25.mlp.fc1.bias', 'visual_encoder.blocks.25.mlp.fc2.weight', 'visual_encoder.blocks.25.mlp.fc2.bias', 'visual_encoder.blocks.26.norm1.weight', 'visual_encoder.blocks.26.norm1.bias', 'visual_encoder.blocks.26.attn.q_bias', 'visual_encoder.blocks.26.attn.v_bias', 'visual_encoder.blocks.26.attn.qkv.weight', 'visual_encoder.blocks.26.attn.proj.weight', 'visual_encoder.blocks.26.attn.proj.bias', 'visual_encoder.blocks.26.norm2.weight', 'visual_encoder.blocks.26.norm2.bias', 'visual_encoder.blocks.26.mlp.fc1.weight', 'visual_encoder.blocks.26.mlp.fc1.bias', 'visual_encoder.blocks.26.mlp.fc2.weight', 'visual_encoder.blocks.26.mlp.fc2.bias', 'visual_encoder.blocks.27.norm1.weight', 'visual_encoder.blocks.27.norm1.bias', 'visual_encoder.blocks.27.attn.q_bias', 'visual_encoder.blocks.27.attn.v_bias', 'visual_encoder.blocks.27.attn.qkv.weight', 'visual_encoder.blocks.27.attn.proj.weight', 'visual_encoder.blocks.27.attn.proj.bias', 'visual_encoder.blocks.27.norm2.weight', 'visual_encoder.blocks.27.norm2.bias', 'visual_encoder.blocks.27.mlp.fc1.weight', 'visual_encoder.blocks.27.mlp.fc1.bias', 'visual_encoder.blocks.27.mlp.fc2.weight', 'visual_encoder.blocks.27.mlp.fc2.bias', 'visual_encoder.blocks.28.norm1.weight', 'visual_encoder.blocks.28.norm1.bias', 'visual_encoder.blocks.28.attn.q_bias', 'visual_encoder.blocks.28.attn.v_bias', 'visual_encoder.blocks.28.attn.qkv.weight', 'visual_encoder.blocks.28.attn.proj.weight', 'visual_encoder.blocks.28.attn.proj.bias', 'visual_encoder.blocks.28.norm2.weight', 'visual_encoder.blocks.28.norm2.bias', 'visual_encoder.blocks.28.mlp.fc1.weight', 'visual_encoder.blocks.28.mlp.fc1.bias', 'visual_encoder.blocks.28.mlp.fc2.weight', 'visual_encoder.blocks.28.mlp.fc2.bias', 'visual_encoder.blocks.29.norm1.weight', 'visual_encoder.blocks.29.norm1.bias', 'visual_encoder.blocks.29.attn.q_bias', 'visual_encoder.blocks.29.attn.v_bias', 'visual_encoder.blocks.29.attn.qkv.weight', 'visual_encoder.blocks.29.attn.proj.weight', 'visual_encoder.blocks.29.attn.proj.bias', 'visual_encoder.blocks.29.norm2.weight', 'visual_encoder.blocks.29.norm2.bias', 'visual_encoder.blocks.29.mlp.fc1.weight', 'visual_encoder.blocks.29.mlp.fc1.bias', 'visual_encoder.blocks.29.mlp.fc2.weight', 'visual_encoder.blocks.29.mlp.fc2.bias', 'visual_encoder.blocks.30.norm1.weight', 'visual_encoder.blocks.30.norm1.bias', 'visual_encoder.blocks.30.attn.q_bias', 'visual_encoder.blocks.30.attn.v_bias', 'visual_encoder.blocks.30.attn.qkv.weight', 'visual_encoder.blocks.30.attn.proj.weight', 'visual_encoder.blocks.30.attn.proj.bias', 'visual_encoder.blocks.30.norm2.weight', 'visual_encoder.blocks.30.norm2.bias', 'visual_encoder.blocks.30.mlp.fc1.weight', 'visual_encoder.blocks.30.mlp.fc1.bias', 'visual_encoder.blocks.30.mlp.fc2.weight', 'visual_encoder.blocks.30.mlp.fc2.bias', 'visual_encoder.blocks.31.norm1.weight', 'visual_encoder.blocks.31.norm1.bias', 'visual_encoder.blocks.31.attn.q_bias', 'visual_encoder.blocks.31.attn.v_bias', 'visual_encoder.blocks.31.attn.qkv.weight', 'visual_encoder.blocks.31.attn.proj.weight', 'visual_encoder.blocks.31.attn.proj.bias', 'visual_encoder.blocks.31.norm2.weight', 'visual_encoder.blocks.31.norm2.bias', 'visual_encoder.blocks.31.mlp.fc1.weight', 'visual_encoder.blocks.31.mlp.fc1.bias', 'visual_encoder.blocks.31.mlp.fc2.weight', 'visual_encoder.blocks.31.mlp.fc2.bias', 'visual_encoder.blocks.32.norm1.weight', 'visual_encoder.blocks.32.norm1.bias', 'visual_encoder.blocks.32.attn.q_bias', 'visual_encoder.blocks.32.attn.v_bias', 'visual_encoder.blocks.32.attn.qkv.weight', 'visual_encoder.blocks.32.attn.proj.weight', 'visual_encoder.blocks.32.attn.proj.bias', 'visual_encoder.blocks.32.norm2.weight', 'visual_encoder.blocks.32.norm2.bias', 'visual_encoder.blocks.32.mlp.fc1.weight', 'visual_encoder.blocks.32.mlp.fc1.bias', 'visual_encoder.blocks.32.mlp.fc2.weight', 'visual_encoder.blocks.32.mlp.fc2.bias', 'visual_encoder.blocks.33.norm1.weight', 'visual_encoder.blocks.33.norm1.bias', 'visual_encoder.blocks.33.attn.q_bias', 'visual_encoder.blocks.33.attn.v_bias', 'visual_encoder.blocks.33.attn.qkv.weight', 'visual_encoder.blocks.33.attn.proj.weight', 'visual_encoder.blocks.33.attn.proj.bias', 'visual_encoder.blocks.33.norm2.weight', 'visual_encoder.blocks.33.norm2.bias', 'visual_encoder.blocks.33.mlp.fc1.weight', 'visual_encoder.blocks.33.mlp.fc1.bias', 'visual_encoder.blocks.33.mlp.fc2.weight', 'visual_encoder.blocks.33.mlp.fc2.bias', 'visual_encoder.blocks.34.norm1.weight', 'visual_encoder.blocks.34.norm1.bias', 'visual_encoder.blocks.34.attn.q_bias', 'visual_encoder.blocks.34.attn.v_bias', 'visual_encoder.blocks.34.attn.qkv.weight', 'visual_encoder.blocks.34.attn.proj.weight', 'visual_encoder.blocks.34.attn.proj.bias', 'visual_encoder.blocks.34.norm2.weight', 'visual_encoder.blocks.34.norm2.bias', 'visual_encoder.blocks.34.mlp.fc1.weight', 'visual_encoder.blocks.34.mlp.fc1.bias', 'visual_encoder.blocks.34.mlp.fc2.weight', 'visual_encoder.blocks.34.mlp.fc2.bias', 'visual_encoder.blocks.35.norm1.weight', 'visual_encoder.blocks.35.norm1.bias', 'visual_encoder.blocks.35.attn.q_bias', 'visual_encoder.blocks.35.attn.v_bias', 'visual_encoder.blocks.35.attn.qkv.weight', 'visual_encoder.blocks.35.attn.proj.weight', 'visual_encoder.blocks.35.attn.proj.bias', 'visual_encoder.blocks.35.norm2.weight', 'visual_encoder.blocks.35.norm2.bias', 'visual_encoder.blocks.35.mlp.fc1.weight', 'visual_encoder.blocks.35.mlp.fc1.bias', 'visual_encoder.blocks.35.mlp.fc2.weight', 'visual_encoder.blocks.35.mlp.fc2.bias', 'visual_encoder.blocks.36.norm1.weight', 'visual_encoder.blocks.36.norm1.bias', 'visual_encoder.blocks.36.attn.q_bias', 'visual_encoder.blocks.36.attn.v_bias', 'visual_encoder.blocks.36.attn.qkv.weight', 'visual_encoder.blocks.36.attn.proj.weight', 'visual_encoder.blocks.36.attn.proj.bias', 'visual_encoder.blocks.36.norm2.weight', 'visual_encoder.blocks.36.norm2.bias', 'visual_encoder.blocks.36.mlp.fc1.weight', 'visual_encoder.blocks.36.mlp.fc1.bias', 'visual_encoder.blocks.36.mlp.fc2.weight', 'visual_encoder.blocks.36.mlp.fc2.bias', 'visual_encoder.blocks.37.norm1.weight', 'visual_encoder.blocks.37.norm1.bias', 'visual_encoder.blocks.37.attn.q_bias', 'visual_encoder.blocks.37.attn.v_bias', 'visual_encoder.blocks.37.attn.qkv.weight', 'visual_encoder.blocks.37.attn.proj.weight', 'visual_encoder.blocks.37.attn.proj.bias', 'visual_encoder.blocks.37.norm2.weight', 'visual_encoder.blocks.37.norm2.bias', 'visual_encoder.blocks.37.mlp.fc1.weight', 'visual_encoder.blocks.37.mlp.fc1.bias', 'visual_encoder.blocks.37.mlp.fc2.weight', 'visual_encoder.blocks.37.mlp.fc2.bias', 'visual_encoder.blocks.38.norm1.weight', 'visual_encoder.blocks.38.norm1.bias', 'visual_encoder.blocks.38.attn.q_bias', 'visual_encoder.blocks.38.attn.v_bias', 'visual_encoder.blocks.38.attn.qkv.weight', 'visual_encoder.blocks.38.attn.proj.weight', 'visual_encoder.blocks.38.attn.proj.bias', 'visual_encoder.blocks.38.norm2.weight', 'visual_encoder.blocks.38.norm2.bias', 'visual_encoder.blocks.38.mlp.fc1.weight', 'visual_encoder.blocks.38.mlp.fc1.bias', 'visual_encoder.blocks.38.mlp.fc2.weight', 'visual_encoder.blocks.38.mlp.fc2.bias', 'llm_model.model.embed_tokens.weight', 'llm_model.model.layers.0.self_attn.q_proj.weight', 'llm_model.model.layers.0.self_attn.k_proj.weight', 'llm_model.model.layers.0.self_attn.v_proj.weight', 'llm_model.model.layers.0.self_attn.o_proj.weight', 'llm_model.model.layers.0.mlp.gate_proj.weight', 'llm_model.model.layers.0.mlp.down_proj.weight', 'llm_model.model.layers.0.mlp.up_proj.weight', 'llm_model.model.layers.0.input_layernorm.weight', 'llm_model.model.layers.0.post_attention_layernorm.weight', 'llm_model.model.layers.1.self_attn.q_proj.weight', 'llm_model.model.layers.1.self_attn.k_proj.weight', 'llm_model.model.layers.1.self_attn.v_proj.weight', 'llm_model.model.layers.1.self_attn.o_proj.weight', 'llm_model.model.layers.1.mlp.gate_proj.weight', 'llm_model.model.layers.1.mlp.down_proj.weight', 'llm_model.model.layers.1.mlp.up_proj.weight', 'llm_model.model.layers.1.input_layernorm.weight', 'llm_model.model.layers.1.post_attention_layernorm.weight', 'llm_model.model.layers.2.self_attn.q_proj.weight', 'llm_model.model.layers.2.self_attn.k_proj.weight', 'llm_model.model.layers.2.self_attn.v_proj.weight', 'llm_model.model.layers.2.self_attn.o_proj.weight', 'llm_model.model.layers.2.mlp.gate_proj.weight', 'llm_model.model.layers.2.mlp.down_proj.weight', 'llm_model.model.layers.2.mlp.up_proj.weight', 'llm_model.model.layers.2.input_layernorm.weight', 'llm_model.model.layers.2.post_attention_layernorm.weight', 'llm_model.model.layers.3.self_attn.q_proj.weight', 'llm_model.model.layers.3.self_attn.k_proj.weight', 'llm_model.model.layers.3.self_attn.v_proj.weight', 'llm_model.model.layers.3.self_attn.o_proj.weight', 'llm_model.model.layers.3.mlp.gate_proj.weight', 'llm_model.model.layers.3.mlp.down_proj.weight', 'llm_model.model.layers.3.mlp.up_proj.weight', 'llm_model.model.layers.3.input_layernorm.weight', 'llm_model.model.layers.3.post_attention_layernorm.weight', 'llm_model.model.layers.4.self_attn.q_proj.weight', 'llm_model.model.layers.4.self_attn.k_proj.weight', 'llm_model.model.layers.4.self_attn.v_proj.weight', 'llm_model.model.layers.4.self_attn.o_proj.weight', 'llm_model.model.layers.4.mlp.gate_proj.weight', 'llm_model.model.layers.4.mlp.down_proj.weight', 'llm_model.model.layers.4.mlp.up_proj.weight', 'llm_model.model.layers.4.input_layernorm.weight', 'llm_model.model.layers.4.post_attention_layernorm.weight', 'llm_model.model.layers.5.self_attn.q_proj.weight', 'llm_model.model.layers.5.self_attn.k_proj.weight', 'llm_model.model.layers.5.self_attn.v_proj.weight', 'llm_model.model.layers.5.self_attn.o_proj.weight', 'llm_model.model.layers.5.mlp.gate_proj.weight', 'llm_model.model.layers.5.mlp.down_proj.weight', 'llm_model.model.layers.5.mlp.up_proj.weight', 'llm_model.model.layers.5.input_layernorm.weight', 'llm_model.model.layers.5.post_attention_layernorm.weight', 'llm_model.model.layers.6.self_attn.q_proj.weight', 'llm_model.model.layers.6.self_attn.k_proj.weight', 'llm_model.model.layers.6.self_attn.v_proj.weight', 'llm_model.model.layers.6.self_attn.o_proj.weight', 'llm_model.model.layers.6.mlp.gate_proj.weight', 'llm_model.model.layers.6.mlp.down_proj.weight', 'llm_model.model.layers.6.mlp.up_proj.weight', 'llm_model.model.layers.6.input_layernorm.weight', 'llm_model.model.layers.6.post_attention_layernorm.weight', 'llm_model.model.layers.7.self_attn.q_proj.weight', 'llm_model.model.layers.7.self_attn.k_proj.weight', 'llm_model.model.layers.7.self_attn.v_proj.weight', 'llm_model.model.layers.7.self_attn.o_proj.weight', 'llm_model.model.layers.7.mlp.gate_proj.weight', 'llm_model.model.layers.7.mlp.down_proj.weight', 'llm_model.model.layers.7.mlp.up_proj.weight', 'llm_model.model.layers.7.input_layernorm.weight', 'llm_model.model.layers.7.post_attention_layernorm.weight', 'llm_model.model.layers.8.self_attn.q_proj.weight', 'llm_model.model.layers.8.self_attn.k_proj.weight', 'llm_model.model.layers.8.self_attn.v_proj.weight', 'llm_model.model.layers.8.self_attn.o_proj.weight', 'llm_model.model.layers.8.mlp.gate_proj.weight', 'llm_model.model.layers.8.mlp.down_proj.weight', 'llm_model.model.layers.8.mlp.up_proj.weight', 'llm_model.model.layers.8.input_layernorm.weight', 'llm_model.model.layers.8.post_attention_layernorm.weight', 'llm_model.model.layers.9.self_attn.q_proj.weight', 'llm_model.model.layers.9.self_attn.k_proj.weight', 'llm_model.model.layers.9.self_attn.v_proj.weight', 'llm_model.model.layers.9.self_attn.o_proj.weight', 'llm_model.model.layers.9.mlp.gate_proj.weight', 'llm_model.model.layers.9.mlp.down_proj.weight', 'llm_model.model.layers.9.mlp.up_proj.weight', 'llm_model.model.layers.9.input_layernorm.weight', 'llm_model.model.layers.9.post_attention_layernorm.weight', 'llm_model.model.layers.10.self_attn.q_proj.weight', 'llm_model.model.layers.10.self_attn.k_proj.weight', 'llm_model.model.layers.10.self_attn.v_proj.weight', 'llm_model.model.layers.10.self_attn.o_proj.weight', 'llm_model.model.layers.10.mlp.gate_proj.weight', 'llm_model.model.layers.10.mlp.down_proj.weight', 'llm_model.model.layers.10.mlp.up_proj.weight', 'llm_model.model.layers.10.input_layernorm.weight', 'llm_model.model.layers.10.post_attention_layernorm.weight', 'llm_model.model.layers.11.self_attn.q_proj.weight', 'llm_model.model.layers.11.self_attn.k_proj.weight', 'llm_model.model.layers.11.self_attn.v_proj.weight', 'llm_model.model.layers.11.self_attn.o_proj.weight', 'llm_model.model.layers.11.mlp.gate_proj.weight', 'llm_model.model.layers.11.mlp.down_proj.weight', 'llm_model.model.layers.11.mlp.up_proj.weight', 'llm_model.model.layers.11.input_layernorm.weight', 'llm_model.model.layers.11.post_attention_layernorm.weight', 'llm_model.model.layers.12.self_attn.q_proj.weight', 'llm_model.model.layers.12.self_attn.k_proj.weight', 'llm_model.model.layers.12.self_attn.v_proj.weight', 'llm_model.model.layers.12.self_attn.o_proj.weight', 'llm_model.model.layers.12.mlp.gate_proj.weight', 'llm_model.model.layers.12.mlp.down_proj.weight', 'llm_model.model.layers.12.mlp.up_proj.weight', 'llm_model.model.layers.12.input_layernorm.weight', 'llm_model.model.layers.12.post_attention_layernorm.weight', 'llm_model.model.layers.13.self_attn.q_proj.weight', 'llm_model.model.layers.13.self_attn.k_proj.weight', 'llm_model.model.layers.13.self_attn.v_proj.weight', 'llm_model.model.layers.13.self_attn.o_proj.weight', 'llm_model.model.layers.13.mlp.gate_proj.weight', 'llm_model.model.layers.13.mlp.down_proj.weight', 'llm_model.model.layers.13.mlp.up_proj.weight', 'llm_model.model.layers.13.input_layernorm.weight', 'llm_model.model.layers.13.post_attention_layernorm.weight', 'llm_model.model.layers.14.self_attn.q_proj.weight', 'llm_model.model.layers.14.self_attn.k_proj.weight', 'llm_model.model.layers.14.self_attn.v_proj.weight', 'llm_model.model.layers.14.self_attn.o_proj.weight', 'llm_model.model.layers.14.mlp.gate_proj.weight', 'llm_model.model.layers.14.mlp.down_proj.weight', 'llm_model.model.layers.14.mlp.up_proj.weight', 'llm_model.model.layers.14.input_layernorm.weight', 'llm_model.model.layers.14.post_attention_layernorm.weight', 'llm_model.model.layers.15.self_attn.q_proj.weight', 'llm_model.model.layers.15.self_attn.k_proj.weight', 'llm_model.model.layers.15.self_attn.v_proj.weight', 'llm_model.model.layers.15.self_attn.o_proj.weight', 'llm_model.model.layers.15.mlp.gate_proj.weight', 'llm_model.model.layers.15.mlp.down_proj.weight', 'llm_model.model.layers.15.mlp.up_proj.weight', 'llm_model.model.layers.15.input_layernorm.weight', 'llm_model.model.layers.15.post_attention_layernorm.weight', 'llm_model.model.layers.16.self_attn.q_proj.weight', 'llm_model.model.layers.16.self_attn.k_proj.weight', 'llm_model.model.layers.16.self_attn.v_proj.weight', 'llm_model.model.layers.16.self_attn.o_proj.weight', 'llm_model.model.layers.16.mlp.gate_proj.weight', 'llm_model.model.layers.16.mlp.down_proj.weight', 'llm_model.model.layers.16.mlp.up_proj.weight', 'llm_model.model.layers.16.input_layernorm.weight', 'llm_model.model.layers.16.post_attention_layernorm.weight', 'llm_model.model.layers.17.self_attn.q_proj.weight', 'llm_model.model.layers.17.self_attn.k_proj.weight', 'llm_model.model.layers.17.self_attn.v_proj.weight', 'llm_model.model.layers.17.self_attn.o_proj.weight', 'llm_model.model.layers.17.mlp.gate_proj.weight', 'llm_model.model.layers.17.mlp.down_proj.weight', 'llm_model.model.layers.17.mlp.up_proj.weight', 'llm_model.model.layers.17.input_layernorm.weight', 'llm_model.model.layers.17.post_attention_layernorm.weight', 'llm_model.model.layers.18.self_attn.q_proj.weight', 'llm_model.model.layers.18.self_attn.k_proj.weight', 'llm_model.model.layers.18.self_attn.v_proj.weight', 'llm_model.model.layers.18.self_attn.o_proj.weight', 'llm_model.model.layers.18.mlp.gate_proj.weight', 'llm_model.model.layers.18.mlp.down_proj.weight', 'llm_model.model.layers.18.mlp.up_proj.weight', 'llm_model.model.layers.18.input_layernorm.weight', 'llm_model.model.layers.18.post_attention_layernorm.weight', 'llm_model.model.layers.19.self_attn.q_proj.weight', 'llm_model.model.layers.19.self_attn.k_proj.weight', 'llm_model.model.layers.19.self_attn.v_proj.weight', 'llm_model.model.layers.19.self_attn.o_proj.weight', 'llm_model.model.layers.19.mlp.gate_proj.weight', 'llm_model.model.layers.19.mlp.down_proj.weight', 'llm_model.model.layers.19.mlp.up_proj.weight', 'llm_model.model.layers.19.input_layernorm.weight', 'llm_model.model.layers.19.post_attention_layernorm.weight', 'llm_model.model.layers.20.self_attn.q_proj.weight', 'llm_model.model.layers.20.self_attn.k_proj.weight', 'llm_model.model.layers.20.self_attn.v_proj.weight', 'llm_model.model.layers.20.self_attn.o_proj.weight', 'llm_model.model.layers.20.mlp.gate_proj.weight', 'llm_model.model.layers.20.mlp.down_proj.weight', 'llm_model.model.layers.20.mlp.up_proj.weight', 'llm_model.model.layers.20.input_layernorm.weight', 'llm_model.model.layers.20.post_attention_layernorm.weight', 'llm_model.model.layers.21.self_attn.q_proj.weight', 'llm_model.model.layers.21.self_attn.k_proj.weight', 'llm_model.model.layers.21.self_attn.v_proj.weight', 'llm_model.model.layers.21.self_attn.o_proj.weight', 'llm_model.model.layers.21.mlp.gate_proj.weight', 'llm_model.model.layers.21.mlp.down_proj.weight', 'llm_model.model.layers.21.mlp.up_proj.weight', 'llm_model.model.layers.21.input_layernorm.weight', 'llm_model.model.layers.21.post_attention_layernorm.weight', 'llm_model.model.layers.22.self_attn.q_proj.weight', 'llm_model.model.layers.22.self_attn.k_proj.weight', 'llm_model.model.layers.22.self_attn.v_proj.weight', 'llm_model.model.layers.22.self_attn.o_proj.weight', 'llm_model.model.layers.22.mlp.gate_proj.weight', 'llm_model.model.layers.22.mlp.down_proj.weight', 'llm_model.model.layers.22.mlp.up_proj.weight', 'llm_model.model.layers.22.input_layernorm.weight', 'llm_model.model.layers.22.post_attention_layernorm.weight', 'llm_model.model.layers.23.self_attn.q_proj.weight', 'llm_model.model.layers.23.self_attn.k_proj.weight', 'llm_model.model.layers.23.self_attn.v_proj.weight', 'llm_model.model.layers.23.self_attn.o_proj.weight', 'llm_model.model.layers.23.mlp.gate_proj.weight', 'llm_model.model.layers.23.mlp.down_proj.weight', 'llm_model.model.layers.23.mlp.up_proj.weight', 'llm_model.model.layers.23.input_layernorm.weight', 'llm_model.model.layers.23.post_attention_layernorm.weight', 'llm_model.model.layers.24.self_attn.q_proj.weight', 'llm_model.model.layers.24.self_attn.k_proj.weight', 'llm_model.model.layers.24.self_attn.v_proj.weight', 'llm_model.model.layers.24.self_attn.o_proj.weight', 'llm_model.model.layers.24.mlp.gate_proj.weight', 'llm_model.model.layers.24.mlp.down_proj.weight', 'llm_model.model.layers.24.mlp.up_proj.weight', 'llm_model.model.layers.24.input_layernorm.weight', 'llm_model.model.layers.24.post_attention_layernorm.weight', 'llm_model.model.layers.25.self_attn.q_proj.weight', 'llm_model.model.layers.25.self_attn.k_proj.weight', 'llm_model.model.layers.25.self_attn.v_proj.weight', 'llm_model.model.layers.25.self_attn.o_proj.weight', 'llm_model.model.layers.25.mlp.gate_proj.weight', 'llm_model.model.layers.25.mlp.down_proj.weight', 'llm_model.model.layers.25.mlp.up_proj.weight', 'llm_model.model.layers.25.input_layernorm.weight', 'llm_model.model.layers.25.post_attention_layernorm.weight', 'llm_model.model.layers.26.self_attn.q_proj.weight', 'llm_model.model.layers.26.self_attn.k_proj.weight', 'llm_model.model.layers.26.self_attn.v_proj.weight', 'llm_model.model.layers.26.self_attn.o_proj.weight', 'llm_model.model.layers.26.mlp.gate_proj.weight', 'llm_model.model.layers.26.mlp.down_proj.weight', 'llm_model.model.layers.26.mlp.up_proj.weight', 'llm_model.model.layers.26.input_layernorm.weight', 'llm_model.model.layers.26.post_attention_layernorm.weight', 'llm_model.model.layers.27.self_attn.q_proj.weight', 'llm_model.model.layers.27.self_attn.k_proj.weight', 'llm_model.model.layers.27.self_attn.v_proj.weight', 'llm_model.model.layers.27.self_attn.o_proj.weight', 'llm_model.model.layers.27.mlp.gate_proj.weight', 'llm_model.model.layers.27.mlp.down_proj.weight', 'llm_model.model.layers.27.mlp.up_proj.weight', 'llm_model.model.layers.27.input_layernorm.weight', 'llm_model.model.layers.27.post_attention_layernorm.weight', 'llm_model.model.layers.28.self_attn.q_proj.weight', 'llm_model.model.layers.28.self_attn.k_proj.weight', 'llm_model.model.layers.28.self_attn.v_proj.weight', 'llm_model.model.layers.28.self_attn.o_proj.weight', 'llm_model.model.layers.28.mlp.gate_proj.weight', 'llm_model.model.layers.28.mlp.down_proj.weight', 'llm_model.model.layers.28.mlp.up_proj.weight', 'llm_model.model.layers.28.input_layernorm.weight', 'llm_model.model.layers.28.post_attention_layernorm.weight', 'llm_model.model.layers.29.self_attn.q_proj.weight', 'llm_model.model.layers.29.self_attn.k_proj.weight', 'llm_model.model.layers.29.self_attn.v_proj.weight', 'llm_model.model.layers.29.self_attn.o_proj.weight', 'llm_model.model.layers.29.mlp.gate_proj.weight', 'llm_model.model.layers.29.mlp.down_proj.weight', 'llm_model.model.layers.29.mlp.up_proj.weight', 'llm_model.model.layers.29.input_layernorm.weight', 'llm_model.model.layers.29.post_attention_layernorm.weight', 'llm_model.model.layers.30.self_attn.q_proj.weight', 'llm_model.model.layers.30.self_attn.k_proj.weight', 'llm_model.model.layers.30.self_attn.v_proj.weight', 'llm_model.model.layers.30.self_attn.o_proj.weight', 'llm_model.model.layers.30.mlp.gate_proj.weight', 'llm_model.model.layers.30.mlp.down_proj.weight', 'llm_model.model.layers.30.mlp.up_proj.weight', 'llm_model.model.layers.30.input_layernorm.weight', 'llm_model.model.layers.30.post_attention_layernorm.weight', 'llm_model.model.layers.31.self_attn.q_proj.weight', 'llm_model.model.layers.31.self_attn.k_proj.weight', 'llm_model.model.layers.31.self_attn.v_proj.weight', 'llm_model.model.layers.31.self_attn.o_proj.weight', 'llm_model.model.layers.31.mlp.gate_proj.weight', 'llm_model.model.layers.31.mlp.down_proj.weight', 'llm_model.model.layers.31.mlp.up_proj.weight', 'llm_model.model.layers.31.input_layernorm.weight', 'llm_model.model.layers.31.post_attention_layernorm.weight', 'llm_model.model.norm.weight', 'llm_model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We associate a model with its preprocessors to make it easier for inference.\n",
    "# You can specify the memory_bank_length and num_frames here.\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '65533'\n",
    "\n",
    "# 初始化分布式进程组\n",
    "dist.init_process_group(backend='nccl', init_method='env://')\n",
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_vicuna_instruct_clip2_malmm\", model_type=\"vicuna7b\", is_eval=True, device=device, memory_bank_length=10, num_frames=20,\n",
    ")\n",
    "checkpoint = \"/t-ng/Deng/py/MMA-LLM/lavis/output/msvd_qa/blip2_vicuna_instruct_clip2_vicuna7b/train/b16_e7_lr0.0001_wd0.05_q32_f20_fb10_freezevit/checkpoint_best.pth\"\n",
    "model.load_checkpoint(checkpoint)\n",
    "model.eval()\n",
    "# model, vis_processors, _ = load_model_and_preprocess(\n",
    "#     name=\"blip2_vicuna_instruct_malmm\", model_type=\"vicuna13b\", is_eval=True, device=device, memory_bank_length=10, num_frames=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load finetuned model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loads the default config from lavis/configs/models/blip2/blip2_instruct_vicuna7b.yaml.\n",
    "# If you want to load a finetuned checkpoints, such as the finetuned model weight of ActivityNet-QA dataset,\n",
    "# you need to first set the load_finetuned=True and specify the finetuned checkpoint path and reload the model again.\n",
    "\n",
    "# load_finetuned: True\n",
    "# finetuned: \"saved_model/ActivityNet_qa/checkpoint_best.pth\"\n",
    "\n",
    "# model, vis_processors, _ = load_model_and_preprocess(\n",
    "#     name=\"blip2_vicuna_instruct_malmm\", model_type=\"vicuna7b\", is_eval=True, device=device, memory_bank_length=10, num_frames=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on long videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default max_num_frames is set to 120 in lavis/configs/models/blip2/blip2_instruct_vicuna7b.yaml. \n",
    "# To test model on long videos, please set the max_num_frames to a larger value and then reload the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructed zero-shot video-to-language generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omelette']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load video by specifying the start_time and end_time\n",
    "video = load_video(vr, start_time=0, end_time=duration, fps=fps, num_frames=120)\n",
    "# prepare the video as model input using the associated processors\n",
    "video = vis_processors[\"eval\"](video).to(device).unsqueeze(0)\n",
    "model.generate({\"image\": video, \"prompt\": \"Question: what is the recipe of this video? Answer:\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online off-the-shelf setting with custom questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['egg']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = load_video(vr, start_time=0, end_time=37, fps=fps, num_frames=120)\n",
    "video = vis_processors[\"eval\"](video).to(device).unsqueeze(0)\n",
    "model.generate({\"image\": video, \"prompt\": \"Question: what will happen for the next 5 seconds? Answer:\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate multiple answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooked', 'grill', 'grilled', 'no', 'yes']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = load_video(vr, start_time=0, end_time=duration, fps=fps, num_frames=20)\n",
    "video = vis_processors[\"eval\"](video).to(device).unsqueeze(0)\n",
    "model.generate({\"image\": video, \"prompt\": \"Question: what does this video show? Answer:\"}, num_captions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
